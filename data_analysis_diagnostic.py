#!/usr/bin/env python3
"""
HANAZONOã‚·ã‚¹ãƒ†ãƒ å®Ÿãƒ‡ãƒ¼ã‚¿è©³ç´°åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
6å¹´é–“ã®å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å†…å®¹ãƒ»å“è³ªãƒ»æ´»ç”¨å¯èƒ½æ€§ã‚’æ­£ç¢ºã«èª¿æŸ»
"""

import os
import json
import glob
from datetime import datetime, timedelta
from pathlib import Path
import sqlite3

class RealDataAnalyzer:
    """å®Ÿãƒ‡ãƒ¼ã‚¿åˆ†æå™¨"""
    
    def __init__(self):
        self.data_directory = "data"
        self.analysis_results = {}
        
    def analyze_all_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æ"""
        print("ğŸ” HANAZONOã‚·ã‚¹ãƒ†ãƒ å®Ÿãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹")
        print("=" * 60)
        
        # 1. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª¿æŸ»
        self._analyze_data_files()
        
        # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª¿æŸ»
        self._analyze_databases()
        
        # 3. å±¥æ­´ãƒ‡ãƒ¼ã‚¿è©³ç´°åˆ†æ
        self._analyze_historical_data()
        
        # 4. ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
        self._evaluate_data_quality()
        
        # 5. MLæ´»ç”¨å¯èƒ½æ€§è©•ä¾¡
        self._evaluate_ml_feasibility()
        
        return self.analysis_results
    
    def _analyze_data_files(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆåˆ†æ"""
        print("\nğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆåˆ†æ:")
        
        if not os.path.exists(self.data_directory):
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.data_directory}")
            return
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«èª¿æŸ»
        json_files = glob.glob(f"{self.data_directory}/*.json")
        csv_files = glob.glob(f"{self.data_directory}/*.csv")
        db_files = glob.glob(f"{self.data_directory}/*.db")
        
        print(f"ğŸ“„ JSONãƒ•ã‚¡ã‚¤ãƒ«: {len(json_files)}ä»¶")
        print(f"ğŸ“Š CSVãƒ•ã‚¡ã‚¤ãƒ«: {len(csv_files)}ä»¶") 
        print(f"ğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {len(db_files)}ä»¶")
        
        # æœ€æ–°ãƒ»æœ€å¤ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        if json_files:
            json_files.sort()
            print(f"ğŸ“… æœ€å¤JSONãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(json_files[0])}")
            print(f"ğŸ“… æœ€æ–°JSONãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(json_files[-1])}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†æ
            total_size = sum(os.path.getsize(f) for f in json_files)
            print(f"ğŸ’¾ ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_size / 1024 / 1024:.2f} MB")
        
        self.analysis_results['file_analysis'] = {
            'json_files': len(json_files),
            'csv_files': len(csv_files),
            'db_files': len(db_files),
            'total_size_mb': total_size / 1024 / 1024 if json_files else 0
        }
    
    def _analyze_databases(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…å®¹åˆ†æ"""
        print("\nğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…å®¹åˆ†æ:")
        
        db_files = glob.glob(f"{self.data_directory}/*.db")
        
        for db_file in db_files:
            print(f"\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {os.path.basename(db_file)}")
            try:
                conn = sqlite3.connect(db_file)
                cursor = conn.cursor()
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§å–å¾—
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
                tables = cursor.fetchall()
                
                for table in tables:
                    table_name = table[0]
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = cursor.fetchone()[0]
                    print(f"  ğŸ“Š {table_name}: {count}ãƒ¬ã‚³ãƒ¼ãƒ‰")
                    
                    # æœ€å¤ãƒ»æœ€æ–°ãƒ‡ãƒ¼ã‚¿ç¢ºèª
                    try:
                        cursor.execute(f"SELECT MIN(timestamp), MAX(timestamp) FROM {table_name}")
                        min_time, max_time = cursor.fetchone()
                        if min_time and max_time:
                            print(f"     æœŸé–“: {min_time} ï½ {max_time}")
                    except:
                        pass
                
                conn.close()
                
            except Exception as e:
                print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
    
    def _analyze_historical_data(self):
        """å±¥æ­´ãƒ‡ãƒ¼ã‚¿è©³ç´°åˆ†æ"""
        print("\nğŸ“ˆ å±¥æ­´ãƒ‡ãƒ¼ã‚¿è©³ç´°åˆ†æ:")
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®åˆ†æ
        json_files = glob.glob(f"{self.data_directory}/lvyuan_data_*.json")
        
        if not json_files:
            print("âŒ lvyuan_data_*.json ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # æœŸé–“åˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†æ
        daily_data = []
        monthly_data = []
        
        for json_file in json_files:
            try:
                with open(json_file, 'r') as f:
                    data = json.load(f)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜æŠ½å‡º
                filename = os.path.basename(json_file)
                if 'lvyuan_data_' in filename:
                    date_str = filename.replace('lvyuan_data_', '').replace('.json', '')
                    
                    # ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª
                    if isinstance(data, list) and len(data) > 50:  # 1æ—¥åˆ†æƒ³å®š
                        daily_data.append({
                            'date': date_str,
                            'records': len(data),
                            'file_size': os.path.getsize(json_file)
                        })
                    elif isinstance(data, dict) or len(data) < 50:  # æœˆæ¬¡æƒ³å®š
                        monthly_data.append({
                            'date': date_str,
                            'records': len(data) if isinstance(data, list) else 1,
                            'file_size': os.path.getsize(json_file)
                        })
                        
            except Exception as e:
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {json_file}: {e}")
        
        print(f"ğŸ“Š æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {len(daily_data)}ä»¶")
        print(f"ğŸ“Š æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {len(monthly_data)}ä»¶")
        
        # æœŸé–“åˆ†æ
        if daily_data:
            daily_data.sort(key=lambda x: x['date'])
            print(f"ğŸ“… æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿æœŸé–“: {daily_data[0]['date']} ï½ {daily_data[-1]['date']}")
            
            # ãƒ‡ãƒ¼ã‚¿å¯†åº¦è¨ˆç®—
            total_daily_records = sum(d['records'] for d in daily_data)
            avg_records_per_day = total_daily_records / len(daily_data)
            print(f"ğŸ“Š å¹³å‡ãƒ¬ã‚³ãƒ¼ãƒ‰/æ—¥: {avg_records_per_day:.1f}")
        
        if monthly_data:
            monthly_data.sort(key=lambda x: x['date'])
            print(f"ğŸ“… æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿æœŸé–“: {monthly_data[0]['date']} ï½ {monthly_data[-1]['date']}")
        
        self.analysis_results['historical_analysis'] = {
            'daily_files': len(daily_data),
            'monthly_files': len(monthly_data),
            'total_records': sum(d['records'] for d in daily_data + monthly_data)
        }
    
    def _evaluate_data_quality(self):
        """ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡"""
        print("\nğŸ¯ ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡:")
        
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æœŸé–“è¨ˆç®—
        json_files = glob.glob(f"{self.data_directory}/lvyuan_data_*.json")
        
        if json_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æœŸé–“æŠ½å‡º
            dates = []
            for json_file in json_files:
                filename = os.path.basename(json_file)
                date_str = filename.replace('lvyuan_data_', '').replace('.json', '')
                try:
                    # YYYYMMDDå½¢å¼æƒ³å®š
                    if len(date_str) == 8:
                        date_obj = datetime.strptime(date_str, '%Y%m%d')
                        dates.append(date_obj)
                except:
                    pass
            
            if dates:
                dates.sort()
                period_start = dates[0]
                period_end = dates[-1]
                total_days = (period_end - period_start).days + 1
                
                print(f"ğŸ“… å®Ÿãƒ‡ãƒ¼ã‚¿æœŸé–“: {period_start.strftime('%Y-%m-%d')} ï½ {period_end.strftime('%Y-%m-%d')}")
                print(f"ğŸ“Š ç·æœŸé–“: {total_days}æ—¥ ({total_days/365.25:.1f}å¹´)")
                print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(json_files)}ä»¶")
                print(f"ğŸ“‰ ãƒ‡ãƒ¼ã‚¿å¯†åº¦: {len(json_files)/total_days*100:.1f}%")
                
                # MLå­¦ç¿’å¯èƒ½æ€§è©•ä¾¡
                ml_feasible = len(json_files) > 365 and (len(json_files)/total_days) > 0.5
                print(f"ğŸ¤– MLå­¦ç¿’å¯èƒ½æ€§: {'âœ… å¯èƒ½' if ml_feasible else 'âš ï¸ é™å®šçš„'}")
                
                self.analysis_results['quality_evaluation'] = {
                    'period_days': total_days,
                    'period_years': total_days/365.25,
                    'data_density': len(json_files)/total_days,
                    'ml_feasible': ml_feasible
                }
    
    def _evaluate_ml_feasibility(self):
        """MLæ´»ç”¨å¯èƒ½æ€§è©³ç´°è©•ä¾¡"""
        print("\nğŸ¤– MLæ´»ç”¨å¯èƒ½æ€§è©³ç´°è©•ä¾¡:")
        
        quality = self.analysis_results.get('quality_evaluation', {})
        period_years = quality.get('period_years', 0)
        data_density = quality.get('data_density', 0)
        
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æœŸé–“: {period_years:.1f}å¹´")
        print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å¯†åº¦: {data_density*100:.1f}%")
        
        # MLå¯èƒ½æ€§ãƒ©ãƒ³ã‚¯åˆ¤å®š
        if period_years >= 3 and data_density >= 0.8:
            ml_rank = "A (é«˜ç²¾åº¦MLå¯èƒ½)"
            expected_accuracy = "90-95%"
        elif period_years >= 2 and data_density >= 0.5:
            ml_rank = "B (ä¸­ç²¾åº¦MLå¯èƒ½)"
            expected_accuracy = "80-90%"
        elif period_years >= 1 and data_density >= 0.3:
            ml_rank = "C (åŸºæœ¬MLå¯èƒ½)"
            expected_accuracy = "70-80%"
        else:
            ml_rank = "D (MLå›°é›£)"
            expected_accuracy = "60-70%"
        
        print(f"ğŸ† MLå¯èƒ½æ€§ãƒ©ãƒ³ã‚¯: {ml_rank}")
        print(f"ğŸ¯ äºˆæƒ³ç²¾åº¦: {expected_accuracy}")
        
        # æ¨å¥¨MLæ©Ÿèƒ½
        if ml_rank.startswith("A"):
            recommended_features = [
                "å®Œå…¨è‡ªå‹•è¨­å®šå¤‰æ›´AI",
                "7æ—¥å…ˆç™ºé›»é‡äºˆæ¸¬",
                "ç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ "
            ]
        elif ml_rank.startswith("B"):
            recommended_features = [
                "3æ—¥å…ˆç™ºé›»é‡äºˆæ¸¬",
                "å­£ç¯€åˆ¥æœ€é©åŒ–",
                "ç°¡æ˜“ç•°å¸¸æ¤œçŸ¥"
            ]
        else:
            recommended_features = [
                "çµ±è¨ˆãƒ™ãƒ¼ã‚¹äºˆæ¸¬",
                "ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–"
            ]
        
        print(f"ğŸ’¡ æ¨å¥¨MLæ©Ÿèƒ½:")
        for feature in recommended_features:
            print(f"   - {feature}")
        
        self.analysis_results['ml_feasibility'] = {
            'rank': ml_rank,
            'expected_accuracy': expected_accuracy,
            'recommended_features': recommended_features
        }

def main():
    """å®Ÿãƒ‡ãƒ¼ã‚¿åˆ†æå®Ÿè¡Œ"""
    analyzer = RealDataAnalyzer()
    results = analyzer.analyze_all_data()
    
    print("\n" + "=" * 60)
    print("ğŸ¯ å®Ÿãƒ‡ãƒ¼ã‚¿åˆ†æå®Œäº†ã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    
    # ç·åˆè©•ä¾¡
    quality = results.get('quality_evaluation', {})
    ml_info = results.get('ml_feasibility', {})
    
    if quality:
        print(f"ğŸ“Š å®Ÿãƒ‡ãƒ¼ã‚¿æœŸé–“: {quality.get('period_years', 0):.1f}å¹´")
        print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å“è³ª: {quality.get('data_density', 0)*100:.1f}%")
    
    if ml_info:
        print(f"ğŸ¤– MLå¯èƒ½æ€§: {ml_info.get('rank', 'Unknown')}")
        print(f"ğŸ¯ äºˆæƒ³ç²¾åº¦: {ml_info.get('expected_accuracy', 'Unknown')}")
    
    print("\nâœ… å®Ÿãƒ‡ãƒ¼ã‚¿åˆ†æå®Œäº†")
    
    return results

if __name__ == "__main__":
    main()
