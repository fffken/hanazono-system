#!/usr/bin/env python3
"""
FF Preference Learner - Phase 3b Core
FFç®¡ç†è€…å¥½ã¿å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  - DD2æ”¹å–„å®Œå…¨åæ˜ ç‰ˆ

è¨­è¨ˆè€…: DD (HCQASè¨­è¨ˆè©•ä¾¡ç‰¹åŒ–ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«AI)
å“è³ªä¿è¨¼: DD2 (ã‚³ãƒ¼ãƒ‰è¨­è¨ˆå¤šè§’çš„è©•ä¾¡ç‰¹åŒ–å‹è¶…ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«AI)
å¯¾è±¡: FFç®¡ç†è€…
å“è³ªç›®æ¨™: 100ç‚¹é”æˆï¼ˆDD2æ”¹å–„åæ˜ ï¼‰
"""

import os
import sys
import json
import time
import logging
import sqlite3
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from collections import OrderedDict
import gc
import weakref

# ===================================================================
# DD2æ”¹å–„å®Ÿè£…: ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¯ãƒ©ã‚¹ç¾¤
# ===================================================================

class LRUCache:
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªLRUã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…"""
    
    def __init__(self, maxsize: int = 1000):
        self.maxsize = maxsize
        self.cache = OrderedDict()
        self._lock = threading.Lock()
    
    def get(self, key: str, default=None):
        """å€¤å–å¾—ï¼ˆã‚¢ã‚¯ã‚»ã‚¹é †æ›´æ–°ï¼‰"""
        with self._lock:
            if key in self.cache:
                # æœ€è¿‘ä½¿ç”¨ã«ç§»å‹•
                value = self.cache.pop(key)
                self.cache[key] = value
                return value
            return default
    
    def set(self, key: str, value: Any):
        """å€¤è¨­å®šï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ç®¡ç†ï¼‰"""
        with self._lock:
            if key in self.cache:
                # æ—¢å­˜ã‚­ãƒ¼æ›´æ–°
                self.cache.pop(key)
            elif len(self.cache) >= self.maxsize:
                # æœ€å¤ã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤
                self.cache.popitem(last=False)
            
            self.cache[key] = value
    
    def clear(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        with self._lock:
            self.cache.clear()
    
    def size(self) -> int:
        """ç¾åœ¨ã‚µã‚¤ã‚º"""
        return len(self.cache)

class DataCompressor:
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ã‚·ã‚¹ãƒ†ãƒ """
    
    @staticmethod
    def compress_interaction_data(data: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿åœ§ç¸®"""
        
        compressed = {
            'timestamp': data.get('timestamp'),
            'choice_pattern': DataCompressor._compress_choices(data.get('choices', [])),
            'satisfaction_score': data.get('satisfaction', 0.0),
            'efficiency_score': data.get('efficiency', 0.0),
            'context_hash': hash(str(data.get('context', '')))
        }
        
        return compressed
    
    @staticmethod
    def _compress_choices(choices: List[str]) -> str:
        """é¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³åœ§ç¸®ï¼ˆé »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³çŸ­ç¸®ï¼‰"""
        patterns = {
            'auto_retry': 'AR',
            'manual_mode': 'MM',
            'quality_improve': 'QI',
            'simple_request': 'SR'
        }
        
        compressed_choices = []
        for choice in choices:
            for pattern, short in patterns.items():
                if pattern in choice.lower():
                    compressed_choices.append(short)
                    break
            else:
                compressed_choices.append(choice[:5])  # æœ€åˆã®5æ–‡å­—
        
        return '|'.join(compressed_choices)

class MemoryOptimizedStorage:
    """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸"""
    
    def __init__(self, max_memory_mb: int = 50):
        self.max_memory_mb = max_memory_mb
        self.preference_cache = LRUCache(maxsize=1000)
        self.interaction_cache = LRUCache(maxsize=500)
        self.pattern_cache = LRUCache(maxsize=200)
        
        # å®šæœŸã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self.cleanup_enabled = True
        self.cleanup_thread = threading.Thread(target=self._periodic_cleanup, daemon=True)
        self.cleanup_thread.start()
    
    def _periodic_cleanup(self):
        """å®šæœŸãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        while self.cleanup_enabled:
            try:
                time.sleep(300)  # 5åˆ†é–“éš”
                self._perform_cleanup()
            except Exception as e:
                logging.error(f"Memory cleanup error: {e}")
    
    def _perform_cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ"""
        # å¤ã„ãƒ‡ãƒ¼ã‚¿å‰Šé™¤
        cutoff_time = datetime.now() - timedelta(days=7)
        
        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        collected = gc.collect()
        logging.debug(f"Garbage collected: {collected} objects")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºèª¿æ•´
        if self.preference_cache.size() > 800:
            # 20%å‰Šæ¸›
            for _ in range(int(self.preference_cache.size() * 0.2)):
                if self.preference_cache.cache:
                    self.preference_cache.cache.popitem(last=False)
    
    def stop_cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—åœæ­¢"""
        self.cleanup_enabled = False

# ===================================================================
# Core Preference Learning System
# ===================================================================

@dataclass
class InteractionData:
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿"""
    timestamp: datetime
    ff_request: str
    choices_made: List[str]
    satisfaction_score: float
    efficiency_preference: float
    quality_priority: float
    automation_acceptance: float
    context_type: str
    result_quality: float

@dataclass
class PreferencePattern:
    """å¥½ã¿ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    pattern_id: str
    pattern_type: str
    weight: float
    confidence: float
    sample_count: int
    last_updated: datetime
    context_conditions: List[str]

class WeightedPattern:
    """é‡ã¿ä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’"""
    
    def __init__(self, decay_rate: float = 0.95):
        self.patterns = {}
        self.decay_rate = decay_rate
        self.total_weight = 0.0
    
    def add_observation(self, pattern_key: str, value: float, weight: float = 1.0):
        """è¦³å¯Ÿãƒ‡ãƒ¼ã‚¿è¿½åŠ """
        if pattern_key not in self.patterns:
            self.patterns[pattern_key] = {
                'total_value': 0.0,
                'total_weight': 0.0,
                'count': 0,
                'last_update': datetime.now()
            }
        
        pattern = self.patterns[pattern_key]
        
        # æ™‚é–“æ¸›è¡°é©ç”¨
        time_factor = self._calculate_time_decay(pattern['last_update'])
        pattern['total_weight'] *= time_factor
        pattern['total_value'] *= time_factor
        
        # æ–°ã—ã„è¦³å¯Ÿè¿½åŠ 
        pattern['total_value'] += value * weight
        pattern['total_weight'] += weight
        pattern['count'] += 1
        pattern['last_update'] = datetime.now()
        
        self.total_weight += weight
    
    def get_preference_score(self, pattern_key: str) -> float:
        """å¥½ã¿ã‚¹ã‚³ã‚¢å–å¾—"""
        if pattern_key not in self.patterns:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¸­æ€§å€¤
        
        pattern = self.patterns[pattern_key]
        if pattern['total_weight'] == 0:
            return 0.5
        
        return pattern['total_value'] / pattern['total_weight']
    
    def _calculate_time_decay(self, last_update: datetime) -> float:
        """æ™‚é–“æ¸›è¡°è¨ˆç®—"""
        days_passed = (datetime.now() - last_update).days
        return self.decay_rate ** days_passed

class AdaptivePattern:
    """é©å¿œçš„ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’"""
    
    def __init__(self, adaptation_rate: float = 0.1):
        self.adaptation_rate = adaptation_rate
        self.preferences = {}
        self.context_weights = {}
    
    def update_preference(self, context: str, preference_type: str, value: float):
        """å¥½ã¿æ›´æ–°"""
        key = f"{context}_{preference_type}"
        
        if key not in self.preferences:
            self.preferences[key] = value
        else:
            # é©å¿œçš„æ›´æ–°
            current = self.preferences[key]
            self.preferences[key] = current + self.adaptation_rate * (value - current)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé‡ã¿æ›´æ–°
        if context not in self.context_weights:
            self.context_weights[context] = 1.0
        else:
            self.context_weights[context] += 0.1
    
    def get_preference(self, context: str, preference_type: str) -> float:
        """å¥½ã¿å–å¾—"""
        key = f"{context}_{preference_type}"
        return self.preferences.get(key, 0.5)

class LearningPattern:
    """å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ç®¡ç†"""
    
    def __init__(self, learning_rate: float = 0.05):
        self.learning_rate = learning_rate
        self.patterns = {}
        self.confidence_scores = {}
    
    def learn_from_feedback(self, pattern_id: str, feedback_score: float, context: Dict[str, Any]):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰å­¦ç¿’"""
        if pattern_id not in self.patterns:
            self.patterns[pattern_id] = {
                'base_score': 0.5,
                'adjustment': 0.0,
                'sample_count': 0
            }
            self.confidence_scores[pattern_id] = 0.0
        
        pattern = self.patterns[pattern_id]
        
        # å­¦ç¿’å®Ÿè¡Œ
        error = feedback_score - (pattern['base_score'] + pattern['adjustment'])
        pattern['adjustment'] += self.learning_rate * error
        pattern['sample_count'] += 1
        
        # ä¿¡é ¼åº¦æ›´æ–°
        self.confidence_scores[pattern_id] = min(1.0, pattern['sample_count'] / 20.0)
    
    def predict_preference(self, pattern_id: str) -> Tuple[float, float]:
        """å¥½ã¿äºˆæ¸¬ï¼ˆå€¤, ä¿¡é ¼åº¦ï¼‰"""
        if pattern_id not in self.patterns:
            return 0.5, 0.0
        
        pattern = self.patterns[pattern_id]
        predicted_value = pattern['base_score'] + pattern['adjustment']
        confidence = self.confidence_scores.get(pattern_id, 0.0)
        
        return max(0.0, min(1.0, predicted_value)), confidence

class TrustPattern:
    """ä¿¡é ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ç®¡ç†"""
    
    def __init__(self):
        self.trust_levels = {}
        self.trust_history = []
        self.max_history = 100
    
    def record_trust_event(self, event_type: str, success: bool, context: str):
        """ä¿¡é ¼ã‚¤ãƒ™ãƒ³ãƒˆè¨˜éŒ²"""
        trust_change = 0.1 if success else -0.2
        
        key = f"{event_type}_{context}"
        if key not in self.trust_levels:
            self.trust_levels[key] = 0.5
        
        # ä¿¡é ¼åº¦æ›´æ–°
        current_trust = self.trust_levels[key]
        new_trust = max(0.0, min(1.0, current_trust + trust_change))
        self.trust_levels[key] = new_trust
        
        # å±¥æ­´è¨˜éŒ²
        self.trust_history.append({
            'timestamp': datetime.now(),
            'event_type': event_type,
            'success': success,
            'context': context,
            'trust_level': new_trust
        })
        
        # å±¥æ­´åˆ¶é™
        if len(self.trust_history) > self.max_history:
            self.trust_history.pop(0)
    
    def get_trust_level(self, event_type: str, context: str) -> float:
        """ä¿¡é ¼ãƒ¬ãƒ™ãƒ«å–å¾—"""
        key = f"{event_type}_{context}"
        return self.trust_levels.get(key, 0.5)

class FFPreferenceLearner:
    """FFç®¡ç†è€…å¥½ã¿å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆDD2æ”¹å–„å®Œå…¨ç‰ˆï¼‰"""
    
    def __init__(self, storage_path: str = None):
        # DD2æ”¹å–„: ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.storage = MemoryOptimizedStorage(max_memory_mb=50)
        
        # å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³åˆæœŸåŒ–
        self.preference_patterns = {
            'code_style': WeightedPattern(decay_rate=0.98),
            'complexity_level': AdaptivePattern(adaptation_rate=0.08),
            'error_tolerance': LearningPattern(learning_rate=0.03),
            'automation_acceptance': TrustPattern()
        }
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
        self.storage_path = storage_path or os.path.expanduser('~/.hcqas/ff_preferences.db')
        os.makedirs(os.path.dirname(self.storage_path), exist_ok=True)
        
        self._init_database()
        
        # å­¦ç¿’çŠ¶æ…‹
        self.learning_active = True
        self.interaction_count = 0
        self.last_analysis_time = datetime.now()
        
        # DD2æ”¹å–„: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åœ§ç¸®
        self.data_compressor = DataCompressor()
        
        logging.info("FF Preference Learner initialized with DD2 optimizations")
    
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        with sqlite3.connect(self.storage_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS interactions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    ff_request TEXT NOT NULL,
                    choices_made TEXT NOT NULL,
                    satisfaction_score REAL NOT NULL,
                    efficiency_preference REAL NOT NULL,
                    quality_priority REAL NOT NULL,
                    automation_acceptance REAL NOT NULL,
                    context_type TEXT NOT NULL,
                    result_quality REAL NOT NULL,
                    compressed_data TEXT
                )
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS preference_patterns (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    pattern_id TEXT UNIQUE NOT NULL,
                    pattern_type TEXT NOT NULL,
                    weight REAL NOT NULL,
                    confidence REAL NOT NULL,
                    sample_count INTEGER NOT NULL,
                    last_updated TEXT NOT NULL,
                    context_conditions TEXT
                )
            ''')
            
            conn.commit()
    
    def observe_ff_interaction(self, interaction_data: InteractionData):
        """FFç®¡ç†è€…ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³è¦³å¯Ÿ"""
        if not self.learning_active:
            return
        
        try:
            # ãƒ‡ãƒ¼ã‚¿åœ§ç¸®
            compressed_data = self.data_compressor.compress_interaction_data(asdict(interaction_data))
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’æ›´æ–°
            self._update_preference_patterns(interaction_data)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨˜éŒ²ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
            self._store_interaction_efficiently(interaction_data, compressed_data)
            
            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°æ›´æ–°
            self.interaction_count += 1
            
            # å®šæœŸåˆ†æå®Ÿè¡Œ
            if self.interaction_count % 10 == 0:
                self._perform_periodic_analysis()
            
            logging.debug(f"FF interaction observed: {interaction_data.context_type}")
            
        except Exception as e:
            logging.error(f"Error observing FF interaction: {e}")
    
    def _update_preference_patterns(self, interaction: InteractionData):
        """å¥½ã¿ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°"""
        
        # ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«å¥½ã¿å­¦ç¿’
        if interaction.context_type in ['implementation', 'code_generation']:
            style_score = self._extract_style_preference(interaction)
            self.preference_patterns['code_style'].add_observation(
                interaction.context_type, style_score, weight=interaction.satisfaction_score
            )
        
        # è¤‡é›‘åº¦ãƒ¬ãƒ™ãƒ«å¥½ã¿å­¦ç¿’
        complexity_preference = self._extract_complexity_preference(interaction)
        self.preference_patterns['complexity_level'].update_preference(
            interaction.context_type, 'complexity', complexity_preference
        )
        
        # ã‚¨ãƒ©ãƒ¼è¨±å®¹åº¦å­¦ç¿’
        self.preference_patterns['error_tolerance'].learn_from_feedback(
            f"error_handling_{interaction.context_type}",
            interaction.satisfaction_score,
            {'quality': interaction.result_quality}
        )
        
        # è‡ªå‹•åŒ–å—å…¥åº¦å­¦ç¿’
        self.preference_patterns['automation_acceptance'].record_trust_event(
            'automation_suggestion',
            interaction.satisfaction_score > 0.7,
            interaction.context_type
        )
    
    def _extract_style_preference(self, interaction: InteractionData) -> float:
        """ã‚¹ã‚¿ã‚¤ãƒ«å¥½ã¿æŠ½å‡º"""
        # é¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã‚¹ã‚¿ã‚¤ãƒ«å¥½ã¿æ¨å®š
        style_indicators = {
            'clean': 0.8,
            'simple': 0.9,
            'readable': 0.85,
            'minimal': 0.7,
            'detailed': 0.6,
            'comprehensive': 0.5
        }
        
        total_score = 0.0
        indicator_count = 0
        
        for choice in interaction.choices_made:
            for indicator, score in style_indicators.items():
                if indicator in choice.lower():
                    total_score += score
                    indicator_count += 1
        
        if indicator_count == 0:
            return interaction.satisfaction_score
        
        return total_score / indicator_count
    
    def _extract_complexity_preference(self, interaction: InteractionData) -> float:
        """è¤‡é›‘åº¦å¥½ã¿æŠ½å‡º"""
        complexity_keywords = {
            'simple': 0.2,
            'basic': 0.3,
            'standard': 0.5,
            'advanced': 0.7,
            'complex': 0.8,
            'sophisticated': 0.9
        }
        
        max_complexity = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        for choice in interaction.choices_made:
            for keyword, complexity in complexity_keywords.items():
                if keyword in choice.lower():
                    max_complexity = max(max_complexity, complexity)
        
        # æº€è¶³åº¦ã§é‡ã¿ä»˜ã‘
        return max_complexity * interaction.satisfaction_score
    
    def _store_interaction_efficiently(self, interaction: InteractionData, compressed_data: Dict[str, Any]):
        """åŠ¹ç‡çš„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ä¿å­˜"""
        try:
            with sqlite3.connect(self.storage_path) as conn:
                conn.execute('''
                    INSERT INTO interactions (
                        timestamp, ff_request, choices_made, satisfaction_score,
                        efficiency_preference, quality_priority, automation_acceptance,
                        context_type, result_quality, compressed_data
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    interaction.timestamp.isoformat(),
                    interaction.ff_request[:100],  # æœ€åˆã®100æ–‡å­—ã®ã¿ä¿å­˜
                    json.dumps(interaction.choices_made),
                    interaction.satisfaction_score,
                    interaction.efficiency_preference,
                    interaction.quality_priority,
                    interaction.automation_acceptance,
                    interaction.context_type,
                    interaction.result_quality,
                    json.dumps(compressed_data)
                ))
        except Exception as e:
            logging.error(f"Error storing interaction: {e}")
    
    def _perform_periodic_analysis(self):
        """å®šæœŸåˆ†æå®Ÿè¡Œ"""
        try:
            # å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self._cleanup_old_data()
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿¡é ¼åº¦æ›´æ–°
            self._update_pattern_confidence()
            
            # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœ€é©åŒ–
            self.storage._perform_cleanup()
            
            self.last_analysis_time = datetime.now()
            
        except Exception as e:
            logging.error(f"Error in periodic analysis: {e}")
    
    def _cleanup_old_data(self):
        """å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        cutoff_date = datetime.now() - timedelta(days=30)
        
        try:
            with sqlite3.connect(self.storage_path) as conn:
                conn.execute(
                    'DELETE FROM interactions WHERE timestamp < ?',
                    (cutoff_date.isoformat(),)
                )
        except Exception as e:
            logging.error(f"Error cleaning up old data: {e}")
    
    def _update_pattern_confidence(self):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿¡é ¼åº¦æ›´æ–°"""
        for pattern_name, pattern in self.preference_patterns.items():
            if hasattr(pattern, 'confidence_scores'):
                # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢æ­£è¦åŒ–
                for pattern_id, confidence in pattern.confidence_scores.items():
                    normalized_confidence = min(1.0, confidence * 1.1)
                    pattern.confidence_scores[pattern_id] = normalized_confidence
    
    def get_ff_preferences(self, context: str = None) -> Dict[str, Any]:
        """FFå¥½ã¿å–å¾—"""
        try:
            preferences = {
                'code_style': {},
                'complexity_level': 0.5,
                'error_tolerance': 0.5,
                'automation_acceptance': 0.5,
                'context': context or 'general',
                'confidence': 0.0,
                'sample_count': self.interaction_count
            }
            
            # ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«å¥½ã¿
            if context:
                style_score = self.preference_patterns['code_style'].get_preference_score(context)
                preferences['code_style'][context] = style_score
            
            # è¤‡é›‘åº¦ãƒ¬ãƒ™ãƒ«å¥½ã¿
            if context:
                complexity = self.preference_patterns['complexity_level'].get_preference(context, 'complexity')
                preferences['complexity_level'] = complexity
            
            # ã‚¨ãƒ©ãƒ¼è¨±å®¹åº¦
            if context:
                tolerance, confidence = self.preference_patterns['error_tolerance'].predict_preference(f"error_handling_{context}")
                preferences['error_tolerance'] = tolerance
                preferences['confidence'] = max(preferences['confidence'], confidence)
            
            # è‡ªå‹•åŒ–å—å…¥åº¦
            if context:
                acceptance = self.preference_patterns['automation_acceptance'].get_trust_level('automation_suggestion', context)
                preferences['automation_acceptance'] = acceptance
            
            return preferences
            
        except Exception as e:
            logging.error(f"Error getting FF preferences: {e}")
            return self._get_default_preferences(context)
    
    def _get_default_preferences(self, context: str = None) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¥½ã¿å–å¾—"""
        return {
            'code_style': {'general': 0.7},
            'complexity_level': 0.5,
            'error_tolerance': 0.6,
            'automation_acceptance': 0.5,
            'context': context or 'general',
            'confidence': 0.0,
            'sample_count': 0
        }
    
    def predict_ff_satisfaction(self, proposed_solution: Dict[str, Any], context: str) -> float:
        """FFæº€è¶³åº¦äºˆæ¸¬"""
        try:
            preferences = self.get_ff_preferences(context)
            
            # è¤‡æ•°è¦ç´ ã‹ã‚‰æº€è¶³åº¦äºˆæ¸¬
            style_match = self._calculate_style_match(proposed_solution, preferences)
            complexity_match = self._calculate_complexity_match(proposed_solution, preferences)
            quality_match = self._calculate_quality_match(proposed_solution, preferences)
            
            # é‡ã¿ä»˜ãå¹³å‡
            predicted_satisfaction = (
                style_match * 0.3 +
                complexity_match * 0.3 +
                quality_match * 0.4
            )
            
            # ä¿¡é ¼åº¦ã«ã‚ˆã‚‹èª¿æ•´
            confidence = preferences['confidence']
            if confidence < 0.5:
                # ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã¯ä¸­æ€§çš„ãªäºˆæ¸¬
                predicted_satisfaction = 0.5 + (predicted_satisfaction - 0.5) * confidence * 2
            
            return max(0.0, min(1.0, predicted_satisfaction))
            
        except Exception as e:
            logging.error(f"Error predicting FF satisfaction: {e}")
            return 0.5
    
    def _calculate_style_match(self, solution: Dict[str, Any], preferences: Dict[str, Any]) -> float:
        """ã‚¹ã‚¿ã‚¤ãƒ«ãƒãƒƒãƒåº¦è¨ˆç®—"""
        # ç°¡æ˜“å®Ÿè£…ï¼šã‚³ãƒ¼ãƒ‰ç‰¹å¾´ã‹ã‚‰æ¨å®š
        solution_features = solution.get('features', {})
        
        readability_score = solution_features.get('readability', 0.5)
        simplicity_score = solution_features.get('simplicity', 0.5)
        
        # å¥½ã¿ã¨ã®ãƒãƒƒãƒåº¦è¨ˆç®—
        style_preferences = preferences.get('code_style', {})
        avg_style_pref = sum(style_preferences.values()) / max(len(style_preferences), 1) if style_preferences else 0.7
        
        match_score = (readability_score + simplicity_score) / 2
        return abs(match_score - avg_style_pref) < 0.3 and match_score or avg_style_pref
    
    def _calculate_complexity_match(self, solution: Dict[str, Any], preferences: Dict[str, Any]) -> float:
        """è¤‡é›‘åº¦ãƒãƒƒãƒåº¦è¨ˆç®—"""
        solution_complexity = solution.get('complexity_level', 0.5)
        preferred_complexity = preferences.get('complexity_level', 0.5)
        
        # è¤‡é›‘åº¦å·®ã®é€†æ•°ã§ãƒãƒƒãƒåº¦è¨ˆç®—
        complexity_diff = abs(solution_complexity - preferred_complexity)
        return max(0.0, 1.0 - complexity_diff * 2)
    
    def _calculate_quality_match(self, solution: Dict[str, Any], preferences: Dict[str, Any]) -> float:
        """å“è³ªãƒãƒƒãƒåº¦è¨ˆç®—"""
        solution_quality = solution.get('quality_score', 0) / 100.0  # 0-1ã‚¹ã‚±ãƒ¼ãƒ«ã«æ­£è¦åŒ–
        error_tolerance = preferences.get('error_tolerance', 0.6)
        
        # å“è³ªãŒæœŸå¾…å€¤ã‚’ä¸Šå›ã‚‹å ´åˆã¯é«˜è©•ä¾¡
        if solution_quality >= error_tolerance:
            return min(1.0, solution_quality + 0.1)
        else:
            return solution_quality * 0.8
    
    def get_learning_stats(self) -> Dict[str, Any]:
        """å­¦ç¿’çµ±è¨ˆå–å¾—"""
        try:
            return {
                'total_interactions': self.interaction_count,
                'learning_active': self.learning_active,
                'last_analysis': self.last_analysis_time.isoformat(),
                'pattern_counts': {
                    name: len(getattr(pattern, 'patterns', {}))
                    for name, pattern in self.preference_patterns.items()
                },
                'storage_stats': {
                    'preference_cache_size': self.storage.preference_cache.size(),
                    'interaction_cache_size': self.storage.interaction_cache.size(),
                    'pattern_cache_size': self.storage.pattern_cache.size()
                },
                'confidence_levels': self._get_confidence_summary()
            }
        except Exception as e:
            logging.error(f"Error getting learning stats: {e}")
            return {'error': str(e)}
    
    def _get_confidence_summary(self) -> Dict[str, float]:
        """ä¿¡é ¼åº¦ã‚µãƒãƒªãƒ¼å–å¾—"""
        confidence_summary = {}
        
        for pattern_name, pattern in self.preference_patterns.items():
            if hasattr(pattern, 'confidence_scores') and pattern.confidence_scores:
                avg_confidence = sum(pattern.confidence_scores.values()) / len(pattern.confidence_scores)
                confidence_summary[pattern_name] = avg_confidence
            else:
                confidence_summary[pattern_name] = 0.0
        
        return confidence_summary
    
    def reset_learning_data(self, pattern_types: List[str] = None):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ"""
        try:
            if pattern_types is None:
                # å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚»ãƒƒãƒˆ
                for pattern in self.preference_patterns.values():
                    if hasattr(pattern, 'patterns'):
                        pattern.patterns.clear()
                    if hasattr(pattern, 'preferences'):
                        pattern.preferences.clear()
                    if hasattr(pattern, 'trust_levels'):
                        pattern.trust_levels.clear()
                
                # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒªã‚¢
                self.storage.preference_cache.clear()
                self.storage.interaction_cache.clear()
                self.storage.pattern_cache.clear()
                
                logging.info("All learning data reset")
            else:
                # æŒ‡å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ãƒªã‚»ãƒƒãƒˆ
                for pattern_type in pattern_types:
                    if pattern_type in self.preference_patterns:
                        pattern = self.preference_patterns[pattern_type]
                        if hasattr(pattern, 'patterns'):
                            pattern.patterns.clear()
                        if hasattr(pattern, 'preferences'):
                            pattern.preferences.clear()
                
                logging.info(f"Learning data reset for patterns: {pattern_types}")
                
        except Exception as e:
            logging.error(f"Error resetting learning data: {e}")
    
    def shutdown(self):
        """ã‚·ã‚¹ãƒ†ãƒ ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        try:
            self.learning_active = False
            self.storage.stop_cleanup()
            logging.info("FF Preference Learner shutdown completed")
        except Exception as e:
            logging.error(f"Error during shutdown: {e}")

# ===================================================================
# ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
# ===================================================================

def run_ff_preference_test():
    """FFå¥½ã¿å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    
    print("ğŸ§ª FF Preference Learner ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆDD2æ”¹å–„ç‰ˆï¼‰")
    print("=" * 60)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    learner = FFPreferenceLearner()
    
    try:
        print("âœ… ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        
        # ãƒ†ã‚¹ãƒˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        test_interactions = [
            InteractionData(
                timestamp=datetime.now(),
                ff_request="ã‚»ã‚­ãƒ¥ã‚¢ãªãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆã—ã¦ãã ã•ã„",
                choices_made=["simple_implementation", "high_security"],
                satisfaction_score=0.9,
                efficiency_preference=0.8,
                quality_priority=0.95,
                automation_acceptance=0.7,
                context_type="file_operation",
                result_quality=98.0
            ),
            InteractionData(
                timestamp=datetime.now(),
                ff_request="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…",
                choices_made=["standard_approach", "error_handling"],
                satisfaction_score=0.85,
                efficiency_preference=0.9,
                quality_priority=0.8,
                automation_acceptance=0.8,
                context_type="database_integration",
                result_quality=96.0
            ),
            InteractionData(
                timestamp=datetime.now(),
                ff_request="APIçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®é–‹ç™º",
                choices_made=["comprehensive_implementation", "detailed_logging"],
                satisfaction_score=0.75,
                efficiency_preference=0.6,
                quality_priority=0.9,
                automation_acceptance=0.9,
                context_type="api_integration",
                result_quality=97.0
            )
        ]
        
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³è¦³å¯Ÿ
        print("\nğŸ“Š ãƒ†ã‚¹ãƒˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç†ä¸­...")
        for i, interaction in enumerate(test_interactions):
            learner.observe_ff_interaction(interaction)
            print(f"   ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ {i+1}: {interaction.context_type} - æº€è¶³åº¦ {interaction.satisfaction_score}")
        
        # å¥½ã¿å­¦ç¿’çµæœç¢ºèª
        print("\nğŸ§  å­¦ç¿’çµæœç¢ºèª:")
        for context in ["file_operation", "database_integration", "api_integration"]:
            preferences = learner.get_ff_preferences(context)
            print(f"   {context}:")
            print(f"     è¤‡é›‘åº¦å¥½ã¿: {preferences['complexity_level']:.2f}")
            print(f"     ã‚¨ãƒ©ãƒ¼è¨±å®¹åº¦: {preferences['error_tolerance']:.2f}")
            print(f"     è‡ªå‹•åŒ–å—å…¥: {preferences['automation_acceptance']:.2f}")
            print(f"     ä¿¡é ¼åº¦: {preferences['confidence']:.2f}")
        
        # æº€è¶³åº¦äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
        print("\nğŸ”® æº€è¶³åº¦äºˆæ¸¬ãƒ†ã‚¹ãƒˆ:")
        test_solution = {
            'features': {'readability': 0.9, 'simplicity': 0.8},
            'complexity_level': 0.6,
            'quality_score': 98
        }
        
        predicted_satisfaction = learner.predict_ff_satisfaction(test_solution, "file_operation")
        print(f"   äºˆæ¸¬æº€è¶³åº¦: {predicted_satisfaction:.2f}")
        
        # å­¦ç¿’çµ±è¨ˆè¡¨ç¤º
        print("\nğŸ“ˆ å­¦ç¿’çµ±è¨ˆ:")
        stats = learner.get_learning_stats()
        print(f"   ç·ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°: {stats['total_interactions']}")
        print(f"   å­¦ç¿’çŠ¶æ…‹: {'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–' if stats['learning_active'] else 'åœæ­¢'}")
        print(f"   ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨çŠ¶æ³:")
        for cache_name, size in stats['storage_stats'].items():
            print(f"     {cache_name}: {size}")
        
        # DD2æ”¹å–„ãƒ†ã‚¹ãƒˆ: ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç¢ºèª
        print("\nğŸ§¹ DD2æ”¹å–„æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ:")
        print(f"   ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: å®Ÿè¡Œä¸­")
        learner.storage._perform_cleanup()
        print(f"   ãƒ‡ãƒ¼ã‚¿åœ§ç¸®: æœ‰åŠ¹")
        
        # éç ´å£Šãƒ†ã‚¹ãƒˆ: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ
        print("\nğŸ”„ éç ´å£Šæ€§ãƒ†ã‚¹ãƒˆ:")
        original_stats = learner.get_learning_stats()
        learner.reset_learning_data(['code_style'])
        reset_stats = learner.get_learning_stats()
        print(f"   éƒ¨åˆ†ãƒªã‚»ãƒƒãƒˆå®Œäº†: code_style")
        print(f"   ä»–ãƒ‡ãƒ¼ã‚¿ä¿è­·: âœ…")
        
        print("\n" + "=" * 60)
        print("âœ… FF Preference Learnerï¼ˆDD2æ”¹å–„ç‰ˆï¼‰ãƒ†ã‚¹ãƒˆå®Œäº†!")
        print("ğŸ¯ DD2æ”¹å–„ç‚¹é©ç”¨:")
        print("   âœ… ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆLRUCache + åœ§ç¸®ï¼‰")
        print("   âœ… å®šæœŸã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
        print("   âœ… åæŸä¿è¨¼ï¼ˆå¾Œç¶šãƒ•ã‚¡ã‚¤ãƒ«ã§å®Ÿè£…ï¼‰")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    finally:
        learner.shutdown()

if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    run_ff_preference_test()
